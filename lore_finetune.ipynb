{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tuning Mistral-7B with LoRA for Code Review and API Deployment\n",
        "\n",
        "This notebook demonstrates the process of fine-tuning the `mistralai/Mistral-7B-Instruct-v0.2`\n",
        "model for the specific task of code review. It utilizes several key technologies:\n",
        "\n",
        "- **Hugging Face Transformers:** For loading the pre-trained model and tokenizer.\n",
        "- **PEFT (Parameter-Efficient Fine-Tuning):** To apply LoRA (Low-Rank Adaptation), which significantly reduces the computational cost of fine-tuning.\n",
        "- **BitsAndBytes:** For 4-bit quantization, making it possible to run large models on consumer-grade GPUs.\n",
        "- **FastAPI & Ngrok:** To deploy the fine-tuned model as a publicly accessible web API.\n",
        "\n",
        "The process is broken down into the following steps:\n",
        "1.  **Setup:** Installing and importing necessary libraries.\n",
        "2.  **Data Preparation:** Loading a custom dataset and tokenizing it for the model.\n",
        "3.  **Model Loading:** Loading the base Mistral model with 4-bit quantization.\n",
        "4.  **LoRA Configuration:** Setting up the LoRA parameters for efficient fine-tuning.\n",
        "5.  **Training:** Running the training job using the `Trainer` class.\n",
        "6.  **Saving & Pushing to Hub:** Saving the trained LoRA adapter and pushing it to the Hugging Face Hub.\n",
        "7.  **API Deployment:** Creating a FastAPI endpoint and exposing it to the web with ngrok."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. SETUP: INSTALL LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, we install all the required Python packages.\n",
        "# - `datasets`: For loading our training data.\n",
        "# - `accelerate`: To enable efficient training on different hardware setups.\n",
        "# - `peft`: The Parameter-Efficient Fine-Tuning library from Hugging Face.\n",
        "# - `bitsandbytes`: For model quantization.\n",
        "# - `transformers`: The core library for working with transformer models.\n",
        "# - `uvicorn`, `fastapi`, `pyngrok`: For creating and deploying the API.\n",
        "#\n",
        "# The `-q` flag is used for a \"quiet\" installation, reducing the output noise.\n",
        "\n",
        "!pip install -q datasets accelerate peft bitsandbytes transformers\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U transformers\n",
        "!pip install -q uvicorn fastapi pyngrok nest-asyncio pydantic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. CONFIGURATION & DATA PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
        "from huggingface_hub import login\n",
        "\n",
        "# --- Hugging Face Configuration ---\n",
        "# It is highly recommended to store your token as a secret in your environment.\n",
        "# Replace the placeholder with your actual Hugging Face token.\n",
        "HF_TOKEN = \"YOUR_HUGGING_FACE_TOKEN_HERE\"\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "# --- Model and Dataset Configuration ---\n",
        "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "DATA_FILE = \"training_data.jsonl\"  # Assumes this file is in the same directory\n",
        "OUTPUT_HUB_REPO = \"your-hf-username/mistral-code-review-lora\" # Replace with your desired repo\n",
        "\n",
        "# --- Load Dataset ---\n",
        "# We load the training data from a JSON Lines file. Each line should be a JSON\n",
        "# object with 'prompt' and 'completion' keys.\n",
        "try:\n",
        "    dataset = load_dataset(\"json\", data_files=DATA_FILE)['train']\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The data file '{DATA_FILE}' was not found.\")\n",
        "    print(\"Please make sure it is uploaded to your Colab/Jupyter environment.\")\n",
        "\n",
        "# --- Tokenizer Initialization ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenizes the combined prompt and completion text from the dataset.\n",
        "    The 'labels' are set to the 'input_ids' which is standard practice for\n",
        "    causal language model fine-tuning.\n",
        "    \"\"\"\n",
        "    # Concatenate prompt and completion for a single training sequence\n",
        "    combined_text = [p + c for p, c in zip(examples['prompt'], examples['completion'])]\n",
        "\n",
        "    tokenized_inputs = tokenizer(\n",
        "        combined_text,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=512, # You can adjust this based on your VRAM and typical sample length\n",
        "    )\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"]\n",
        "    return tokenized_inputs\n",
        "\n",
        "# --- Process Dataset ---\n",
        "# Apply the tokenization function to the entire dataset.\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. MODEL LOADING AND QUANTIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here, we load the base Mistral model. To make it fit into memory, we use\n",
        "# 4-bit quantization via the BitsAndBytes library.\n",
        "\n",
        "# --- BitsAndBytes Configuration (4-bit quantization) ---\n",
        "# This configuration enables loading the model in 4-bit precision, which\n",
        "# significantly reduces the memory footprint.\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",          # Use the \"Normal Float 4\" quantization type\n",
        "    bnb_4bit_use_double_quant=True,    # Improves quantization precision\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 # Use bfloat16 for computations\n",
        ")\n",
        "\n",
        "# --- Load Base Model ---\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\" # Automatically maps model layers to available devices (GPU/CPU)\n",
        ")\n",
        "\n",
        "# --- Prepare Model for LoRA ---\n",
        "# This crucial step prepares a quantized model for PEFT (LoRA) training. It freezes\n",
        "# the original model weights and adds the trainable LoRA adapters.\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. LoRA CONFIGURATION AND MODEL TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now we define the LoRA configuration and set up the training arguments.\n",
        "\n",
        "# --- LoRA Configuration ---\n",
        "# Configures the parameters for Low-Rank Adaptation.\n",
        "lora_config = LoraConfig(\n",
        "    r=8,                             # Rank of the update matrices (a lower rank means fewer trainable parameters)\n",
        "    lora_alpha=16,                   # Alpha parameter for scaling LoRA weights\n",
        "    target_modules=[\"q_proj\", \"v_proj\"], # Apply LoRA to the query and value projections in the attention layers\n",
        "    lora_dropout=0.1,                # Dropout for regularization\n",
        "    bias=\"none\",                     # Do not train bias terms\n",
        "    task_type=\"CAUSAL_LM\"            # Specify the task type\n",
        ")\n",
        "\n",
        "# --- Apply PEFT to the Model ---\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters() \n",
        "\n",
        "# --- Training Arguments ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"outputs\",\n",
        "    per_device_train_batch_size=2,   # Adjust based on your GPU memory\n",
        "    gradient_accumulation_steps=4,   # Effectively increases batch size\n",
        "    num_train_epochs=1,              # A single epoch is often enough for fine-tuning\n",
        "    logging_dir=\"logs\",\n",
        "    save_total_limit=1,              # Only keep the latest checkpoint\n",
        "    save_steps=50,                   # Save a checkpoint every 50 steps\n",
        "    logging_steps=10,                # Log training metrics every 10 steps\n",
        "    report_to=\"none\",                # Can be set to \"wandb\", \"tensorboard\", etc.\n",
        "    fp16=True                        # Use mixed-precision training for speed\n",
        ")\n",
        "\n",
        "# --- Initialize Trainer ---\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "# --- Start Training ---\n",
        "print(\"Starting model training...\")\n",
        "trainer.train()\n",
        "print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. SAVE AND PUSH MODEL TO HUB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After training, we save the LoRA adapter locally and then push it to the\n",
        "# Hugging Face Hub for easy access and deployment.\n",
        "\n",
        "# --- Save LoRA Adapter and Tokenizer Locally ---\n",
        "LORA_ADAPTER_DIR = \"mistral-code-review-lora-adapter\"\n",
        "model.save_pretrained(LORA_ADAPTER_DIR)\n",
        "tokenizer.save_pretrained(LORA_ADAPTER_DIR)\n",
        "print(f\"Model adapter and tokenizer saved to '{LORA_ADAPTER_DIR}'.\")\n",
        "\n",
        "# --- Push to Hugging Face Hub ---\n",
        "# This pushes only the trained adapter weights, not the full model.\n",
        "model.push_to_hub(OUTPUT_HUB_REPO, use_auth_token=True)\n",
        "tokenizer.push_to_hub(OUTPUT_HUB_REPO, use_auth_token=True)\n",
        "print(f\"Model and tokenizer pushed to Hugging Face Hub: {OUTPUT_HUB_REPO}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. API DEPLOYMENT WITH FASTAPI AND NGROK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This final section sets up a web server to serve our fine-tuned model and\n",
        "# uses ngrok to create a public URL for it.\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "# --- Initialize FastAPI App ---\n",
        "app = FastAPI(\n",
        "    title=\"Code Review API\",\n",
        "    description=\"An API to get AI-powered code reviews using a fine-tuned Mistral model.\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "\n",
        "# --- Load Fine-Tuned Model for Inference ---\n",
        "# We reload the base model and then apply our trained LoRA adapter on top.\n",
        "print(\"Loading model for inference...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "inference_model = PeftModel.from_pretrained(base_model, OUTPUT_HUB_REPO)\n",
        "inference_tokenizer = AutoTokenizer.from_pretrained(OUTPUT_HUB_REPO)\n",
        "print(\"Inference model and tokenizer loaded successfully.\")\n",
        "\n",
        "# --- Define API Request Schema ---\n",
        "class ReviewRequest(BaseModel):\n",
        "    \"\"\"Defines the structure for a code review request.\"\"\"\n",
        "    diff: str\n",
        "\n",
        "# --- Define API Endpoint ---\n",
        "@app.post(\"/review\")\n",
        "def review_code(request: ReviewRequest):\n",
        "    \"\"\"\n",
        "    Accepts a code diff and returns a constructive review.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = f\"\"\"You are an expert code reviewer. Here's a code diff from a pull request:\n",
        "        ```diff\n",
        "        {request.diff}\n",
        "        ```\n",
        "        Please write a constructive review summarizing:\n",
        "        - What the code does\n",
        "        - What changed\n",
        "        - How it can be improved (if anything)\n",
        "        - Whether it follows best practices\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Generating review for the provided diff...\")\n",
        "    inputs = inference_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate the review\n",
        "    output = inference_model.generate(**inputs, max_new_tokens=400, temperature=0.7)\n",
        "    review = inference_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(\"Review generated successfully.\")\n",
        "\n",
        "    return {\"review\": review}\n",
        "\n",
        "# --- Ngrok Tunneling Setup ---\n",
        "# Replace with your actual ngrok authtoken.\n",
        "NGROK_AUTH_TOKEN = \"YOUR_NGROK_AUTH_TOKEN_HERE\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# `nest_asyncio` is needed to run `uvicorn` in a notebook environment\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- Expose API using Ngrok ---\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Public API URL: {public_url}\")\n",
        "print(\"Send a POST request to the /review endpoint of the URL above.\")\n",
        "print(\"Example using curl:\")\n",
        "print(f\"curl -X POST \\\"{public_url}/review\\\" -H \\\"Content-Type: application/json\\\" -d '{{\\\"diff\\\": \\\"your_code_diff_here\\\"}}'\")\n",
        "\n",
        "\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm (3.9.6)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
